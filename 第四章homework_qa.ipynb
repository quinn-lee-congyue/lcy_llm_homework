{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f7d8703-6037-4b47-905b-59cdc2285611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n",
      "2024-03-01 15:20:08.106003: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-01 15:20:08.210013: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-01 15:20:08.802601: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-01 15:20:10.288036: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "import random\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import subprocess\n",
    "import collections\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import ClassLabel, Sequence,load_dataset,load_metric\n",
    "from IPython.display import display,HTML\n",
    "from transformers import AutoTokenizer,AutoModelForQuestionAnswering,TrainingArguments,Trainer,default_data_collator, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031122cf-3abc-49e0-9fc1-335d76974a77",
   "metadata": {},
   "source": [
    "### 任务二：QA任务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e61e40e-a2e4-498a-8d6e-3e69e0bdfc30",
   "metadata": {},
   "source": [
    "1. 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "292dc1ae-8fb6-40fb-8587-e40908ac2ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据使用的模型和GPU资源情况，调整以下关键参数\n",
    "squad_v2 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76b06a1b-b92b-47d5-ab44-f3038ee11abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e2b736b-c14d-4729-b9ab-b8d347a81507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dfca12-59f2-40f5-b43d-c9a581d00a93",
   "metadata": {},
   "source": [
    "训练集train：8.8万样本；验证机validation：1.1万样本集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "859366f2-9faa-44f5-abd0-b1461194dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化数据\n",
    "def show_random_elements(dataset, num_examples=2):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a44217a4-39dc-4a72-8f73-adfa6d21d2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56d130df17492d1400aabbc0</td>\n",
       "      <td>IPod</td>\n",
       "      <td>The iPod has also been credited with accelerating shifts within the music industry. The iPod's popularization of digital music storage allows users to abandon listening to entire albums and instead be able to choose specific singles which hastened the end of the Album Era in popular music.</td>\n",
       "      <td>The ease of collecting singles with the iPod and iTunes is credited with ending what \"era\" in pop music?</td>\n",
       "      <td>{'text': ['the Album Era'], 'answer_start': [259]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57342937d058e614000b6a66</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Portuguese cuisine is diverse. The Portuguese consume a lot of dry cod (bacalhau in Portuguese), for which there are hundreds of recipes. There are more than enough bacalhau dishes for each day of the year. Two other popular fish recipes are grilled sardines and caldeirada, a potato-based stew that can be made from several types of fish. Typical Portuguese meat recipes, that may be made out of beef, pork, lamb, or chicken, include cozido à portuguesa, feijoada, frango de churrasco, leitão (roast suckling pig) and carne de porco à alentejana. A very popular northern dish is the arroz de sarrabulho (rice stewed in pigs blood) or the arroz de cabidela (rice and chickens meat stewed in chickens blood).</td>\n",
       "      <td>What is caldeirada?</td>\n",
       "      <td>{'text': ['a potato-based stew that can be made from several types of fish'], 'answer_start': [275]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbb5ba7-a504-4170-b012-fc83abc02809",
   "metadata": {},
   "source": [
    "2.数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8c74ee7-2ed1-402e-9e13-24bd9e83f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础模型\n",
    "model_checkpoint = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1c40d50-4349-4fd6-a700-974d137063c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "max_length = 384 \n",
    "doc_stride = 128 \n",
    "pad_on_right = tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a73c8630-9657-4207-bef8-6ae17cfb1aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "def prepare_train_features(examples):\n",
    "    # 一些问题的左侧可能有很多空白字符，这对我们没有用，而且会导致上下文的截断失败\n",
    "    # （标记化的问题将占用大量空间）。因此，我们删除左侧的空白字符。\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # 使用截断和填充对我们的示例进行标记化，但保留溢出部分，使用步幅（stride）。\n",
    "    # 当上下文很长时，这会导致一个示例可能提供多个特征，其中每个特征的上下文都与前一个特征的上下文有一些重叠。\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # 由于一个示例可能给我们提供多个特征（如果它具有很长的上下文），我们需要一个从特征到其对应示例的映射。这个键就提供了这个映射关系。\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # 偏移映射将为我们提供从令牌到原始上下文中的字符位置的映射。这将帮助我们计算开始位置和结束位置。\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # 让我们为这些示例进行标记！\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # 我们将使用 CLS 特殊 token 的索引来标记不可能的答案。\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # 获取与该示例对应的序列（以了解上下文和问题是什么）。\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # 一个示例可以提供多个跨度，这是包含此文本跨度的示例的索引。\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # 如果没有给出答案，则将cls_index设置为答案。\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # 答案在文本中的开始和结束字符索引。\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # 当前跨度在文本中的开始令牌索引。\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # 当前跨度在文本中的结束令牌索引。\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # 检测答案是否超出跨度（在这种情况下，该特征的标签将使用CLS索引）。\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # 否则，将token_start_index和token_end_index移到答案的两端。\n",
    "                # 注意：如果答案是最后一个单词（边缘情况），我们可以在最后一个偏移之后继续。\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2f3b324-1ae4-4020-8624-2c66f22dd90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对所有的数据集应用函数\n",
    "tokenized_datasets = datasets.map(prepare_train_features,\n",
    "                                  batched=True,\n",
    "                                  remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "467f7db3-3a74-4ce9-87da-890c1c2c4b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 88524\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 10784\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa443e1-a4d8-4713-95d4-70e46adad660",
   "metadata": {},
   "source": [
    "新增答案在上下文中起始和结尾位置坐标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f404661-5374-4ec5-8ab0-7e3d10c06231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>start_positions</th>\n",
       "      <th>end_positions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 2129, 2079, 11572, 2817, 1037, 5776, 1005, 1055, 11311, 2291, 11595, 1999, 5057, 8146, 1029, 102, 3188, 2000, 1996, 8259, 1997, 15403, 2013, 1996, 3802, 24335, 10091, 7117, 10047, 23041, 2483, 1010, 2029, 2003, 3763, 2005, 1000, 11819, 1000, 1025, 2220, 11572, 7356, 11595, 2008, 2052, 2101, 2022, 10003, 2004, 6827, 6177, 1997, 1996, 11311, 2291, 1012, 1996, 2590, 1048, 24335, 8458, 9314, 11595, 1997, 1996, 11311, 2291, 2024, 1996, 15177, 7606, 1998, 5923, 24960, 1010, 1998, 2708, 1048, 24335, 21890, 4588, 14095, 2107, 2004, 11867, 24129, 1010, 6197, 12146, 1010, 1048, 24335, 8458, 6470, 1010, 1048, 24335, 8458, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "      <td>146</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 2054, 2003, 5298, 1005, 1055, 2087, 20151, 2103, 1029, 102, 5298, 1006, 1045, 1013, 100, 1013, 1007, 1006, 13796, 1024, 100, 1010, 9092, 21369, 1007, 2003, 1037, 2110, 2284, 1999, 1996, 8252, 2142, 2163, 1012, 5298, 2003, 1996, 21460, 2922, 1998, 1996, 5550, 2087, 20151, 1997, 1996, 2753, 2142, 2163, 1012, 5298, 2003, 11356, 2011, 5612, 1998, 3448, 2000, 1996, 2167, 1010, 2167, 3792, 2000, 1996, 2264, 1010, 4108, 1010, 6041, 1010, 1998, 5900, 2000, 1996, 2148, 1010, 1998, 6751, 1998, 5284, 2000, 1996, 2225, 1012, 1996, 19682, 4020, 16083, 1996, 2789, 2112, 1997, 1996, 2110, 1010, 1998, 1996, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(tokenized_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526c95ad-4377-4ff8-8d31-afd43d94e5fc",
   "metadata": {},
   "source": [
    "3.模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc68d1ff-4379-41bc-9276-02f6bb536c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 模型微调\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 训练超参数\n",
    "batch_size=64\n",
    "model_dir = f\"models/{model_checkpoint}-finetuned-squad\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# 数据整理器：将训练数据整理为批次数据，用于模型训练时的批次处理\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18144581-39e1-4365-a0b9-4a356ffc2859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# 实例化训练器\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df5f053-920a-4476-a228-1a4b8a1995e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e306aa-6b76-4363-9088-5fb0d0250fec",
   "metadata": {},
   "source": [
    "4. 模型评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e8c88c-c70f-449d-8d49-26ad816aa023",
   "metadata": {},
   "source": [
    "对验证集进行预处理，获取答案在上下文中的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7549795-954e-48d9-8a2b-42e6c4c01828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # 去掉左侧空白符\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "    # 截断或者填充\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation = \"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length = max_length,\n",
    "        stride = doc_stride,\n",
    "        return_overflowing_tokens = True,\n",
    "        return_offsets_mapping = True,\n",
    "        padding = \"max_length\",\n",
    "    )\n",
    "\n",
    "    #获取当前的起始和结束位置\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # 获取与该实例对应的序列（以了解哪些是上下文，哪些是问题）\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        #一个示例可以产生几个文本段，这里是包含该文本段的示例的索引\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        #将不属于上下文的偏移映射设置为none，以便容易确定一个令牌位置是否属于上下文\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k]==context_index else None)\n",
    "            for k,o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1868cb62-2146-4235-a7cf-bc9e1c4e76dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a66e70db-d6b7-4b10-aa83-3fcb0e00142e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
       "    num_rows: 10784\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50ea0edd-3aad-4c93-a52b-e4bff14346d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>offset_mapping</th>\n",
       "      <th>example_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 2054, 3128, 2708, 7837, 2000, 2413, 3481, 1998, 5561, 16400, 1029, 102, 1999, 1996, 3500, 1997, 23810, 1010, 2703, 16400, 2139, 2474, 15451, 9077, 2001, 2445, 3094, 1997, 1037, 1016, 1010, 2199, 1011, 2158, 2486, 1997, 16017, 2015, 2139, 2474, 3884, 1998, 6505, 1012, 2010, 4449, 2020, 2000, 4047, 1996, 2332, 1005, 1055, 2455, 1999, 1996, 4058, 3028, 2013, 1996, 2329, 1012, 16400, 2628, 1996, 2799, 2008, 8292, 10626, 2239, 2018, 17715, 2041, 2176, 2086, 3041, 1010, 2021, 2073, 8292, 10626, 2239, 2018, 3132, 1996, 2501, 1997, 2413, 4447, 2000, 1996, 8940, 1997, 2599, 7766, 1010, 16400, 3833, 1998, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "      <td>[None, None, None, None, None, None, None, None, None, None, None, None, None, [0, 2], [3, 6], [7, 13], [14, 16], [17, 21], [21, 22], [23, 27], [28, 33], [34, 36], [37, 39], [40, 43], [43, 46], [47, 50], [51, 56], [57, 64], [65, 67], [68, 69], [70, 71], [71, 72], [72, 75], [75, 76], [76, 79], [80, 85], [86, 88], [89, 95], [95, 96], [97, 99], [100, 102], [103, 109], [110, 113], [114, 121], [121, 122], [123, 126], [127, 133], [134, 138], [139, 141], [142, 149], [150, 153], [154, 158], [158, 159], [159, 160], [161, 165], [166, 168], [169, 172], [173, 177], [178, 184], [185, 189], [190, 193], [194, 201], [201, 202], [203, 208], [209, 217], [218, 221], [222, 227], [228, 232], [233, 235], [235, 238], [238, 240], [241, 244], [245, 251], [252, 255], [256, 260], [261, 266], [267, 274], [274, 275], [276, 279], [280, 285], [286, 288], [288, 291], [291, 293], [294, 297], [298, 305], [306, 309], [310, 316], [317, 319], [320, 326], [327, 333], [334, 336], [337, 340], [341, 347], [348, 350], [351, 355], [356, 362], [362, 363], [364, 369], [370, 381], [382, 385], ...]</td>\n",
       "      <td>5733ea04d058e614000b6598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 2429, 2000, 7312, 1010, 2065, 1060, 1998, 1061, 2064, 2022, 13332, 2011, 1996, 2168, 9896, 2059, 1060, 10438, 2054, 3853, 1999, 3276, 2000, 1061, 1029, 102, 2116, 11619, 4280, 2024, 4225, 2478, 1996, 4145, 1997, 1037, 7312, 1012, 1037, 7312, 2003, 1037, 8651, 1997, 2028, 3291, 2046, 2178, 3291, 1012, 2009, 19566, 1996, 11900, 9366, 1997, 1037, 3291, 2108, 2012, 2560, 2004, 3697, 2004, 2178, 3291, 1012, 2005, 6013, 1010, 2065, 1037, 3291, 1060, 2064, 2022, 13332, 2478, 2019, 9896, 2005, 1061, 1010, 1060, 2003, 2053, 2062, 3697, 2084, 1061, 1010, 1998, 2057, 2360, 2008, 1060, 13416, 2000, 1061, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "      <td>[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, [0, 4], [5, 15], [16, 23], [24, 27], [28, 35], [36, 41], [42, 45], [46, 53], [54, 56], [57, 58], [59, 68], [68, 69], [70, 71], [72, 81], [82, 84], [85, 86], [87, 101], [102, 104], [105, 108], [109, 116], [117, 121], [122, 129], [130, 137], [137, 138], [139, 141], [142, 150], [151, 154], [155, 163], [164, 170], [171, 173], [174, 175], [176, 183], [184, 189], [190, 192], [193, 198], [199, 201], [202, 211], [212, 214], [215, 222], [223, 230], [230, 231], [232, 235], [236, 244], [244, 245], [246, 248], [249, 250], [251, 258], [259, 260], [261, 264], [265, 267], [268, 274], [275, 280], [281, 283], [284, 293], [294, 297], [298, 299], [299, 300], [301, 302], [303, 305], [306, 308], [309, 313], [314, 323], [324, 328], [329, 330], [330, 331], [332, 335], [336, 338], [339, 342], [343, 347], [348, 349], [350, 357], [358, 360], [361, 362], ...]</td>\n",
       "      <td>56e1c9bfe3433e1400423194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c3cd129-84a4-47d0-983f-e3aaf22e2fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 利用原始模型对验证集预测\n",
    "raw_predictions = trainer.predict(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "986ac82d-21f3-402a-8d9a-e1b803a3cf80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8778a2bc-3797-4654-a39b-04bec5ed94d9",
   "metadata": {},
   "source": [
    "将模型输出的答案在上下文中的起始和结尾位置的概率分布映射为答案的概率分布，其中概率为起始位置概率+结尾位置概率，取最大的20个输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53ce453c-87ab-421a-8685-992ae2d20e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c7186cb-1d8a-4183-979e-ab8dbdbfd88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # 构建一个从示例到其对应特征的映射。\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # 我们需要填充的字典。\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # 日志记录。\n",
    "    print(f\"正在后处理 {len(examples)} 个示例的预测，这些预测分散在 {len(features)} 个特征中。\")\n",
    "\n",
    "    # 遍历所有示例！\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # 这些是与当前示例关联的特征的索引。\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # 仅在squad_v2为True时使用。\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # 遍历与当前示例关联的所有特征。\n",
    "        for feature_index in feature_indices:\n",
    "            # 我们获取模型对这个特征的预测。\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # 这将允许我们将logits中的某些位置映射到原始上下文中的文本跨度。\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # 更新最小空预测。\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # 浏览所有的最佳开始和结束logits，为 `n_best_size` 个最佳选择。\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # 不考虑超出范围的答案，原因是索引超出范围或对应于输入ID的部分不在上下文中。\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # 不考虑长度小于0或大于max_answer_length的答案。\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # 在极少数情况下我们没有一个非空预测，我们创建一个假预测以避免失败。\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # 选择我们的最终答案：最佳答案或空答案（仅适用于squad_v2）\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "695f508b-805b-4a80-bc07-22b524029e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在后处理 10570 个示例的预测，这些预测分散在 10784 个特征中。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662f5a2b438b48dc8c33beda4a7a17d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "624cc234-497e-4837-a788-8a719613c644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1897/2905994612.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/load.py:753: FutureWarning: The repository for squad contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/squad/squad.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "606d0d89-31f4-4d74-a9d3-1acf7a18cf05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0.1892147587511826, 'f1': 7.183507430240401}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if squad_v2:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
    "else:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b11111-65e6-4a7c-a033-1540dd91accc",
   "metadata": {},
   "source": [
    "对原始模型进行评估，得到最终f1得分为7.1835。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cbfaa2-f13a-41df-bc88-ac0e4536844e",
   "metadata": {},
   "source": [
    "导入全量样本集微调的模型进行评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb80badd-1558-4bb6-8a8d-9bf7728da2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aac70e42-0d3a-4605-8133-1fd64fac6a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trained_trainer = Trainer(\n",
    "    trained_model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08060ead-506a-4687-92f0-11c0afda5d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 利用微调模型对验证集预测\n",
    "trained_predictions = trained_trainer.predict(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81b1d999-0be0-40c3-b99d-f77732ab1095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在后处理 10570 个示例的预测，这些预测分散在 10784 个特征中。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7472f8babdf8449bb3dd6d7a69e04942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_trained_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, trained_predictions.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89a504e2-ad2b-42f7-a755-4200293c11aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 74.99526963103122, 'f1': 83.81339494622097}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if squad_v2:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_trained_predictions.items()]\n",
    "else:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_trained_predictions.items()]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faef009-1341-47ec-b279-b97a8f942846",
   "metadata": {},
   "source": [
    "使用原始模型distilbert-base-uncased对squad数据集进行预测评估，模型f1得分为7.1835；通过使用squad数据集全量数据对原始模型进行微调，模型f1得分提升到83.8134."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f5e8d66-4b4d-4ea2-b304-82acb6eda2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='169' max='169' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [169/169 01:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 5.987925052642822,\n",
       " 'eval_runtime': 108.3884,\n",
       " 'eval_samples_per_second': 99.494,\n",
       " 'eval_steps_per_second': 1.559}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eaf10b-101d-4ec3-b413-6c6eb3127345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
